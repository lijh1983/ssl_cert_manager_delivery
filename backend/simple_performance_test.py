#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆæ€§èƒ½ä¼˜åŒ–æµ‹è¯•è„šæœ¬
æµ‹è¯•åŸºæœ¬çš„æ€§èƒ½ä¼˜åŒ–åŠŸèƒ½
"""

import time
import asyncio
import json
from typing import Dict, Any, List


class SimpleCacheManager:
    """ç®€åŒ–ç‰ˆç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, max_size: int = 1000):
        self.cache = {}
        self.access_times = {}
        self.max_size = max_size
    
    def get(self, key: str):
        if key in self.cache:
            self.access_times[key] = time.time()
            return self.cache[key]
        return None
    
    def set(self, key: str, value: Any):
        if len(self.cache) >= self.max_size:
            # ç®€å•çš„LRUæ·˜æ±°
            oldest_key = min(self.access_times.keys(), key=lambda k: self.access_times[k])
            del self.cache[oldest_key]
            del self.access_times[oldest_key]
        
        self.cache[key] = value
        self.access_times[key] = time.time()
    
    def clear(self):
        self.cache.clear()
        self.access_times.clear()
    
    def get_stats(self):
        return {
            'total_items': len(self.cache),
            'max_size': self.max_size,
            'memory_usage_kb': len(str(self.cache)) / 1024
        }


class SimplePagination:
    """ç®€åŒ–ç‰ˆåˆ†é¡µåŠ©æ‰‹"""
    
    @staticmethod
    def paginate(data: List[Any], page: int = 1, per_page: int = 20):
        total = len(data)
        total_pages = (total + per_page - 1) // per_page
        start = (page - 1) * per_page
        end = start + per_page
        
        return {
            'items': data[start:end],
            'pagination': {
                'page': page,
                'per_page': per_page,
                'total': total,
                'total_pages': total_pages,
                'has_prev': page > 1,
                'has_next': page < total_pages
            }
        }


class PerformanceTester:
    """æ€§èƒ½æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.results = {}
        self.cache_manager = SimpleCacheManager()
    
    def test_cache_performance(self):
        """æµ‹è¯•ç¼“å­˜æ€§èƒ½"""
        print("\n" + "="*50)
        print("ğŸš€ ç¼“å­˜æ€§èƒ½æµ‹è¯•")
        print("="*50)
        
        # æµ‹è¯•æ•°æ®
        test_data = [f"test_data_{i}" for i in range(1000)]
        
        # æµ‹è¯•æ— ç¼“å­˜æ€§èƒ½
        start_time = time.time()
        results_no_cache = []
        for i, data in enumerate(test_data):
            # æ¨¡æ‹Ÿå¤æ‚è®¡ç®—
            result = data.upper() + str(i) + "_processed"
            results_no_cache.append(result)
        no_cache_time = time.time() - start_time
        
        # æµ‹è¯•æœ‰ç¼“å­˜æ€§èƒ½ï¼ˆé¦–æ¬¡ï¼‰
        start_time = time.time()
        results_with_cache = []
        for i, data in enumerate(test_data):
            cache_key = f"processed_{i}"
            cached_result = self.cache_manager.get(cache_key)
            if cached_result is None:
                result = data.upper() + str(i) + "_processed"
                self.cache_manager.set(cache_key, result)
                results_with_cache.append(result)
            else:
                results_with_cache.append(cached_result)
        with_cache_time = time.time() - start_time
        
        # æµ‹è¯•ç¼“å­˜å‘½ä¸­æ€§èƒ½ï¼ˆç¬¬äºŒæ¬¡ï¼‰
        start_time = time.time()
        results_cache_hit = []
        for i in range(len(test_data)):
            cache_key = f"processed_{i}"
            cached_result = self.cache_manager.get(cache_key)
            results_cache_hit.append(cached_result)
        cache_hit_time = time.time() - start_time
        
        print(f"ğŸ“Š ç¼“å­˜æ€§èƒ½æµ‹è¯•ç»“æœ:")
        print(f"  æ— ç¼“å­˜å¤„ç†æ—¶é—´: {no_cache_time:.4f}s")
        print(f"  æœ‰ç¼“å­˜å¤„ç†æ—¶é—´: {with_cache_time:.4f}s")
        print(f"  ç¼“å­˜å‘½ä¸­æ—¶é—´: {cache_hit_time:.4f}s")
        
        if cache_hit_time > 0:
            improvement = ((no_cache_time - cache_hit_time) / no_cache_time * 100)
            print(f"  æ€§èƒ½æå‡: {improvement:.1f}%")
        else:
            print(f"  æ€§èƒ½æå‡: >99%")
        
        # ç¼“å­˜ç»Ÿè®¡
        stats = self.cache_manager.get_stats()
        print(f"  ç¼“å­˜ç»Ÿè®¡: {stats}")
        
        self.results['cache'] = {
            'no_cache_time': no_cache_time,
            'with_cache_time': with_cache_time,
            'cache_hit_time': cache_hit_time,
            'improvement': improvement if cache_hit_time > 0 else 99.9,
            'stats': stats
        }
    
    def test_pagination_performance(self):
        """æµ‹è¯•åˆ†é¡µæ€§èƒ½"""
        print("\n" + "="*50)
        print("ğŸ“„ åˆ†é¡µæ€§èƒ½æµ‹è¯•")
        print("="*50)
        
        # åˆ›å»ºå¤§æ•°æ®é›†
        large_dataset = [f"record_{i}" for i in range(10000)]
        
        # æµ‹è¯•ä¸åŒåˆ†é¡µå¤§å°çš„æ€§èƒ½
        page_sizes = [10, 20, 50, 100]
        pagination_results = {}
        
        for page_size in page_sizes:
            start_time = time.time()
            
            # æ¨¡æ‹Ÿåˆ†é¡µæŸ¥è¯¢
            result = SimplePagination.paginate(large_dataset, page=1, per_page=page_size)
            
            end_time = time.time()
            duration = end_time - start_time
            
            pagination_results[page_size] = {
                'duration': duration,
                'items_count': len(result['items']),
                'total_pages': result['pagination']['total_pages']
            }
            
            print(f"  é¡µå¤§å° {page_size}: {duration:.6f}s, {len(result['items'])} æ¡è®°å½•")
        
        # æµ‹è¯•å¤§é¡µç çš„æ€§èƒ½
        start_time = time.time()
        large_page_result = SimplePagination.paginate(large_dataset, page=100, per_page=20)
        large_page_time = time.time() - start_time
        
        print(f"ğŸ“Š åˆ†é¡µæ€§èƒ½æµ‹è¯•ç»“æœ:")
        print(f"  å¤§é¡µç æŸ¥è¯¢æ—¶é—´: {large_page_time:.6f}s")
        print(f"  æ€»è®°å½•æ•°: {len(large_dataset)}")
        
        self.results['pagination'] = {
            'page_size_results': pagination_results,
            'large_page_time': large_page_time,
            'total_records': len(large_dataset)
        }
    
    async def test_async_performance(self):
        """æµ‹è¯•å¼‚æ­¥æ€§èƒ½"""
        print("\n" + "="*50)
        print("âš¡ å¼‚æ­¥æ€§èƒ½æµ‹è¯•")
        print("="*50)
        
        async def mock_io_operation(delay: float, data: str):
            """æ¨¡æ‹ŸIOæ“ä½œ"""
            await asyncio.sleep(delay)
            return f"processed_{data}"
        
        # æµ‹è¯•æ•°æ®
        test_items = [f"item_{i}" for i in range(10)]
        delay = 0.05  # 50mså»¶è¿Ÿ
        
        # æµ‹è¯•ä¸²è¡Œæ‰§è¡Œ
        start_time = time.time()
        serial_results = []
        for item in test_items:
            result = await mock_io_operation(delay, item)
            serial_results.append(result)
        serial_time = time.time() - start_time
        
        # æµ‹è¯•å¹¶è¡Œæ‰§è¡Œ
        start_time = time.time()
        tasks = [mock_io_operation(delay, item) for item in test_items]
        parallel_results = await asyncio.gather(*tasks)
        parallel_time = time.time() - start_time
        
        # æµ‹è¯•æ‰¹é‡å¹¶è¡Œæ‰§è¡Œï¼ˆé™åˆ¶å¹¶å‘æ•°ï¼‰
        async def batch_process(items: List[str], batch_size: int = 3):
            results = []
            for i in range(0, len(items), batch_size):
                batch = items[i:i + batch_size]
                batch_tasks = [mock_io_operation(delay, item) for item in batch]
                batch_results = await asyncio.gather(*batch_tasks)
                results.extend(batch_results)
            return results
        
        start_time = time.time()
        batch_results = await batch_process(test_items)
        batch_time = time.time() - start_time
        
        print(f"ğŸ“Š å¼‚æ­¥æ€§èƒ½æµ‹è¯•ç»“æœ:")
        print(f"  ä¸²è¡Œæ‰§è¡Œæ—¶é—´: {serial_time:.4f}s")
        print(f"  å¹¶è¡Œæ‰§è¡Œæ—¶é—´: {parallel_time:.4f}s")
        print(f"  æ‰¹é‡æ‰§è¡Œæ—¶é—´: {batch_time:.4f}s")
        
        serial_improvement = ((serial_time - parallel_time) / serial_time * 100)
        batch_improvement = ((serial_time - batch_time) / serial_time * 100)
        
        print(f"  å¹¶è¡Œæ€§èƒ½æå‡: {serial_improvement:.1f}%")
        print(f"  æ‰¹é‡æ€§èƒ½æå‡: {batch_improvement:.1f}%")
        
        self.results['async'] = {
            'serial_time': serial_time,
            'parallel_time': parallel_time,
            'batch_time': batch_time,
            'parallel_improvement': serial_improvement,
            'batch_improvement': batch_improvement
        }
    
    def test_data_processing_performance(self):
        """æµ‹è¯•æ•°æ®å¤„ç†æ€§èƒ½"""
        print("\n" + "="*50)
        print("ğŸ”„ æ•°æ®å¤„ç†æ€§èƒ½æµ‹è¯•")
        print("="*50)
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        raw_data = [
            {'id': i, 'name': f'item_{i}', 'value': i * 2, 'category': f'cat_{i % 5}'}
            for i in range(5000)
        ]
        
        # æµ‹è¯•åˆ—è¡¨æ¨å¯¼å¼
        start_time = time.time()
        list_comp_result = [
            {'id': item['id'], 'processed_name': item['name'].upper(), 'doubled_value': item['value'] * 2}
            for item in raw_data if item['value'] > 100
        ]
        list_comp_time = time.time() - start_time
        
        # æµ‹è¯•ä¼ ç»Ÿå¾ªç¯
        start_time = time.time()
        loop_result = []
        for item in raw_data:
            if item['value'] > 100:
                processed_item = {
                    'id': item['id'],
                    'processed_name': item['name'].upper(),
                    'doubled_value': item['value'] * 2
                }
                loop_result.append(processed_item)
        loop_time = time.time() - start_time
        
        # æµ‹è¯•filter + map
        start_time = time.time()
        filtered_data = filter(lambda x: x['value'] > 100, raw_data)
        map_result = list(map(
            lambda item: {
                'id': item['id'],
                'processed_name': item['name'].upper(),
                'doubled_value': item['value'] * 2
            },
            filtered_data
        ))
        map_time = time.time() - start_time
        
        print(f"ğŸ“Š æ•°æ®å¤„ç†æ€§èƒ½æµ‹è¯•ç»“æœ:")
        print(f"  åˆ—è¡¨æ¨å¯¼å¼: {list_comp_time:.4f}s, {len(list_comp_result)} æ¡è®°å½•")
        print(f"  ä¼ ç»Ÿå¾ªç¯: {loop_time:.4f}s, {len(loop_result)} æ¡è®°å½•")
        print(f"  filter+map: {map_time:.4f}s, {len(map_result)} æ¡è®°å½•")
        
        # è®¡ç®—æœ€å¿«çš„æ–¹æ³•
        times = {'list_comp': list_comp_time, 'loop': loop_time, 'map': map_time}
        fastest = min(times, key=times.get)
        print(f"  æœ€å¿«æ–¹æ³•: {fastest}")
        
        self.results['data_processing'] = {
            'list_comp_time': list_comp_time,
            'loop_time': loop_time,
            'map_time': map_time,
            'fastest_method': fastest,
            'processed_count': len(list_comp_result)
        }
    
    def generate_report(self):
        """ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ğŸ“‹ æ€§èƒ½ä¼˜åŒ–æµ‹è¯•æŠ¥å‘Š")
        print("="*60)
        
        print("\nğŸ¯ æµ‹è¯•æ€»ç»“:")
        
        if 'cache' in self.results:
            cache_improvement = self.results['cache']['improvement']
            print(f"  â€¢ ç¼“å­˜ä¼˜åŒ–: æ€§èƒ½æå‡ {cache_improvement:.1f}%")
        
        if 'pagination' in self.results:
            total_records = self.results['pagination']['total_records']
            print(f"  â€¢ åˆ†é¡µä¼˜åŒ–: å¤„ç† {total_records} æ¡è®°å½•")
        
        if 'async' in self.results:
            parallel_improvement = self.results['async']['parallel_improvement']
            batch_improvement = self.results['async']['batch_improvement']
            print(f"  â€¢ å¼‚æ­¥ä¼˜åŒ–: å¹¶è¡Œæå‡ {parallel_improvement:.1f}%, æ‰¹é‡æå‡ {batch_improvement:.1f}%")
        
        if 'data_processing' in self.results:
            fastest = self.results['data_processing']['fastest_method']
            print(f"  â€¢ æ•°æ®å¤„ç†: æœ€å¿«æ–¹æ³•æ˜¯ {fastest}")
        
        print("\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        
        # æ ¹æ®æµ‹è¯•ç»“æœç”Ÿæˆå»ºè®®
        if 'cache' in self.results:
            if self.results['cache']['improvement'] > 80:
                print("  â€¢ ç¼“å­˜æ•ˆæœæ˜¾è‘—ï¼Œå»ºè®®åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å¯ç”¨")
            elif self.results['cache']['improvement'] > 50:
                print("  â€¢ ç¼“å­˜æœ‰ä¸€å®šæ•ˆæœï¼Œå»ºè®®ä¼˜åŒ–ç¼“å­˜ç­–ç•¥")
            else:
                print("  â€¢ ç¼“å­˜æ•ˆæœä¸æ˜æ˜¾ï¼Œå»ºè®®æ£€æŸ¥ç¼“å­˜å®ç°")
        
        if 'async' in self.results:
            if self.results['async']['parallel_improvement'] > 70:
                print("  â€¢ å¼‚æ­¥å¹¶è¡Œæ•ˆæœæ˜¾è‘—ï¼Œå»ºè®®åœ¨IOå¯†é›†å‹æ“ä½œä¸­ä½¿ç”¨")
            else:
                print("  â€¢ è€ƒè™‘ä½¿ç”¨æ‰¹é‡å¤„ç†æ¥å¹³è¡¡å¹¶å‘å’Œèµ„æºä½¿ç”¨")
        
        print("\nğŸ“Š è¯¦ç»†æ•°æ®:")
        print(json.dumps(self.results, indent=2, ensure_ascii=False))
        
        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        with open('simple_performance_report.json', 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        print(f"\nâœ… æ€§èƒ½æµ‹è¯•å®Œæˆï¼ŒæŠ¥å‘Šå·²ä¿å­˜åˆ° simple_performance_report.json")


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ‰ å¼€å§‹SSLè¯ä¹¦ç®¡ç†ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–æµ‹è¯•")
    print("ğŸ“ è¿™æ˜¯ä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬çš„æ€§èƒ½æµ‹è¯•ï¼Œç”¨äºéªŒè¯åŸºæœ¬ä¼˜åŒ–æ•ˆæœ")
    
    tester = PerformanceTester()
    
    try:
        # æ‰§è¡Œå„é¡¹æ€§èƒ½æµ‹è¯•
        tester.test_cache_performance()
        tester.test_pagination_performance()
        tester.test_data_processing_performance()
        await tester.test_async_performance()
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        tester.generate_report()
        
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    asyncio.run(main())
