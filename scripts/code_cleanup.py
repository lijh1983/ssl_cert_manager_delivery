#!/usr/bin/env python3
"""
SSLè¯ä¹¦ç®¡ç†ç³»ç»Ÿ - ä»£ç æ¸…ç†å’Œä¼˜åŒ–è„šæœ¬
å…¨é¢åˆ†æé¡¹ç›®ç»“æ„ï¼Œè¯†åˆ«å¹¶æ¸…ç†å†—ä½™æ–‡ä»¶ï¼Œä¼˜åŒ–é¡¹ç›®ç»„ç»‡ç»“æ„
"""

import os
import sys
import json
import shutil
import logging
import datetime
from pathlib import Path
from typing import Dict, List, Set, Tuple
from collections import defaultdict

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('cleanup.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class ProjectCleaner:
    """é¡¹ç›®æ¸…ç†å™¨"""
    
    def __init__(self, project_root: str = '.'):
        """åˆå§‹åŒ–æ¸…ç†å™¨"""
        self.project_root = Path(project_root).resolve()
        self.cleanup_report = {
            'timestamp': datetime.datetime.now().isoformat(),
            'deleted_files': [],
            'kept_files': [],
            'analysis': {},
            'warnings': [],
            'errors': []
        }
        
        # å®šä¹‰æ–‡ä»¶ç±»å‹å’Œæ¨¡å¼
        self.config_extensions = {'.yml', '.yaml', '.json', '.toml', '.ini', '.conf'}
        self.script_extensions = {'.sh', '.bat', '.ps1'}
        self.doc_extensions = {'.md', '.txt', '.rst'}
        self.docker_patterns = {'Dockerfile', 'docker-compose'}
        self.temp_patterns = {'.bak', '.tmp', '.old', '~', '.swp', '.swo'}
        
        # æ ¸å¿ƒåŠŸèƒ½æ–‡ä»¶ï¼ˆä¸èƒ½åˆ é™¤ï¼‰
        self.core_files = {
            'backend/src/app.py',
            'backend/src/models/database.py',
            'backend/src/models/certificate.py',
            'backend/src/models/user.py',
            'backend/src/models/server.py',
            'backend/src/models/alert.py',
            'backend/src/routes/auth.py',
            'backend/src/routes/certificates.py',
            'backend/src/routes/monitoring.py',
            'backend/src/utils/cert_utils.py',
            'backend/src/utils/acme_client.py',
            'frontend/src/main.ts',
            'frontend/src/App.vue',
            'frontend/package.json',
            'backend/requirements.txt',
            'README.md'
        }
        
        # ç”Ÿäº§ç¯å¢ƒå¿…éœ€æ–‡ä»¶
        self.production_files = {
            'docker-compose.production.yml',
            'docker-compose.mysql.yml',
            '.env.production.example',
            'nginx/nginx.conf',
            'nginx/conf.d/ssl-manager-production.conf',
            'backend/Dockerfile.production',
            'backend/config/gunicorn.conf.py',
            'docs/production_deployment.md',
            'docs/mysql_deployment.md'
        }
    
    def analyze_project_structure(self) -> Dict:
        """åˆ†æé¡¹ç›®ç»“æ„"""
        logger.info("åˆ†æé¡¹ç›®ç»“æ„...")
        
        analysis = {
            'total_files': 0,
            'file_types': defaultdict(int),
            'directories': [],
            'duplicates': [],
            'empty_files': [],
            'large_files': [],
            'potential_redundant': []
        }
        
        for root, dirs, files in os.walk(self.project_root):
            root_path = Path(root)
            relative_root = root_path.relative_to(self.project_root)
            
            # è·³è¿‡éšè—ç›®å½•å’Œå¸¸è§çš„å¿½ç•¥ç›®å½•
            dirs[:] = [d for d in dirs if not d.startswith('.') and d not in {
                'node_modules', '__pycache__', '.git', '.vscode', '.idea',
                'dist', 'build', 'target', 'coverage'
            }]
            
            analysis['directories'].append(str(relative_root))
            
            for file in files:
                if file.startswith('.'):
                    continue
                
                file_path = root_path / file
                relative_path = file_path.relative_to(self.project_root)
                
                analysis['total_files'] += 1
                
                # æ–‡ä»¶ç±»å‹ç»Ÿè®¡
                suffix = file_path.suffix.lower()
                analysis['file_types'][suffix] += 1
                
                # æ£€æŸ¥ç©ºæ–‡ä»¶
                try:
                    if file_path.stat().st_size == 0:
                        analysis['empty_files'].append(str(relative_path))
                except OSError:
                    continue
                
                # æ£€æŸ¥å¤§æ–‡ä»¶ï¼ˆ>10MBï¼‰
                try:
                    if file_path.stat().st_size > 10 * 1024 * 1024:
                        analysis['large_files'].append({
                            'path': str(relative_path),
                            'size': file_path.stat().st_size
                        })
                except OSError:
                    continue
        
        # è½¬æ¢defaultdictä¸ºæ™®é€šdictä»¥ä¾¿JSONåºåˆ—åŒ–
        analysis['file_types'] = dict(analysis['file_types'])
        self.cleanup_report['analysis'] = analysis
        return analysis
    
    def find_duplicate_files(self) -> List[Tuple[str, List[str]]]:
        """æŸ¥æ‰¾é‡å¤æ–‡ä»¶"""
        logger.info("æŸ¥æ‰¾é‡å¤æ–‡ä»¶...")
        
        file_contents = defaultdict(list)
        
        for root, dirs, files in os.walk(self.project_root):
            # è·³è¿‡éšè—ç›®å½•
            dirs[:] = [d for d in dirs if not d.startswith('.')]
            
            for file in files:
                if file.startswith('.'):
                    continue
                
                file_path = Path(root) / file
                relative_path = file_path.relative_to(self.project_root)
                
                try:
                    # å¯¹äºå°æ–‡ä»¶ï¼Œæ¯”è¾ƒå†…å®¹
                    if file_path.stat().st_size < 1024 * 1024:  # 1MBä»¥ä¸‹
                        with open(file_path, 'rb') as f:
                            content_hash = hash(f.read())
                        file_contents[content_hash].append(str(relative_path))
                except (OSError, UnicodeDecodeError):
                    continue
        
        # æ‰¾å‡ºé‡å¤çš„æ–‡ä»¶
        duplicates = []
        for content_hash, paths in file_contents.items():
            if len(paths) > 1:
                duplicates.append((content_hash, paths))
        
        return duplicates
    
    def identify_redundant_docker_files(self) -> List[str]:
        """è¯†åˆ«å†—ä½™çš„Dockeræ–‡ä»¶"""
        logger.info("è¯†åˆ«å†—ä½™çš„Dockeræ–‡ä»¶...")
        
        docker_files = []
        for root, dirs, files in os.walk(self.project_root):
            dirs[:] = [d for d in dirs if not d.startswith('.')]
            
            for file in files:
                file_path = Path(root) / file
                relative_path = file_path.relative_to(self.project_root)
                
                if (file.startswith('Dockerfile') or 
                    file.startswith('docker-compose') or
                    file.endswith('.dockerfile')):
                    docker_files.append(str(relative_path))
        
        # åˆ†æå“ªäº›æ˜¯å†—ä½™çš„
        redundant = []
        
        # æ£€æŸ¥å¤šä¸ªdocker-composeæ–‡ä»¶
        compose_files = [f for f in docker_files if 'docker-compose' in f]
        
        # ä¿ç•™ç”Ÿäº§ç¯å¢ƒå’ŒMySQLé…ç½®ï¼Œå…¶ä»–å¯èƒ½å†—ä½™
        essential_compose = {
            'docker-compose.production.yml',
            'docker-compose.mysql.yml',
            'docker-compose.yml'  # å¼€å‘ç¯å¢ƒä¸»æ–‡ä»¶
        }
        
        for compose_file in compose_files:
            if Path(compose_file).name not in essential_compose:
                redundant.append(compose_file)
        
        # æ£€æŸ¥å¤šä¸ªDockerfile
        dockerfile_files = [f for f in docker_files if 'Dockerfile' in f and 'docker-compose' not in f]
        
        # ä¿ç•™ç”Ÿäº§ç¯å¢ƒDockerfile
        essential_dockerfiles = {
            'backend/Dockerfile.production',
            'backend/Dockerfile',
            'frontend/Dockerfile'
        }
        
        for dockerfile in dockerfile_files:
            if dockerfile not in essential_dockerfiles:
                redundant.append(dockerfile)
        
        return redundant
    
    def identify_redundant_docs(self) -> List[str]:
        """è¯†åˆ«å†—ä½™çš„æ–‡æ¡£æ–‡ä»¶"""
        logger.info("è¯†åˆ«å†—ä½™çš„æ–‡æ¡£æ–‡ä»¶...")
        
        doc_files = []
        for root, dirs, files in os.walk(self.project_root):
            dirs[:] = [d for d in dirs if not d.startswith('.')]
            
            for file in files:
                if any(file.endswith(ext) for ext in self.doc_extensions):
                    file_path = Path(root) / file
                    relative_path = file_path.relative_to(self.project_root)
                    doc_files.append(str(relative_path))
        
        # åˆ†ææ–‡æ¡£é‡å¤æ€§
        redundant = []
        
        # æ£€æŸ¥æ ¹ç›®å½•ä¸‹çš„å¤šä¸ªéƒ¨ç½²æ–‡æ¡£
        root_docs = [f for f in doc_files if '/' not in f]
        deployment_docs = [f for f in root_docs if 'DEPLOYMENT' in f.upper()]
        
        # ä¿ç•™ä¸»è¦çš„éƒ¨ç½²æ–‡æ¡£ï¼Œç§»é™¤é‡å¤çš„
        if len(deployment_docs) > 2:
            # ä¿ç•™æœ€é‡è¦çš„ä¸¤ä¸ª
            essential_deployment = ['DEPLOYMENT.md', 'DEPLOYMENT_LOCAL.md']
            for doc in deployment_docs:
                if doc not in essential_deployment:
                    redundant.append(doc)
        
        # æ£€æŸ¥è®¾è®¡æ–‡æ¡£çš„é‡å¤
        design_docs = [f for f in doc_files if 'design' in f.lower()]
        if len(design_docs) > 3:
            # å¯èƒ½æœ‰é‡å¤çš„è®¾è®¡æ–‡æ¡£
            for doc in design_docs[3:]:
                redundant.append(doc)
        
        return redundant
    
    def identify_redundant_configs(self) -> List[str]:
        """è¯†åˆ«å†—ä½™çš„é…ç½®æ–‡ä»¶"""
        logger.info("è¯†åˆ«å†—ä½™çš„é…ç½®æ–‡ä»¶...")
        
        config_files = []
        for root, dirs, files in os.walk(self.project_root):
            dirs[:] = [d for d in dirs if not d.startswith('.')]
            
            for file in files:
                if any(file.endswith(ext) for ext in self.config_extensions):
                    file_path = Path(root) / file
                    relative_path = file_path.relative_to(self.project_root)
                    config_files.append(str(relative_path))
        
        redundant = []
        
        # æ£€æŸ¥nginxé…ç½®é‡å¤
        nginx_configs = [f for f in config_files if 'nginx' in f and f.endswith('.conf')]
        if len(nginx_configs) > 3:
            # ä¿ç•™ä¸»è¦çš„nginxé…ç½®
            essential_nginx = [
                'nginx/nginx.conf',
                'nginx/conf.d/ssl-manager-production.conf',
                'frontend/nginx.conf'
            ]
            for config in nginx_configs:
                if config not in essential_nginx:
                    redundant.append(config)
        
        return redundant
    
    def check_file_dependencies(self, file_path: str) -> List[str]:
        """æ£€æŸ¥æ–‡ä»¶ä¾èµ–å…³ç³»"""
        dependencies = []
        
        try:
            full_path = self.project_root / file_path
            
            if full_path.suffix in {'.py', '.js', '.ts', '.vue'}:
                with open(full_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # ç®€å•çš„ä¾èµ–æ£€æŸ¥
                if 'import' in content or 'require' in content:
                    dependencies.append('has_imports')
                
                if 'export' in content or 'module.exports' in content:
                    dependencies.append('exports_symbols')
            
            elif full_path.suffix in {'.yml', '.yaml'}:
                # Docker composeæˆ–å…¶ä»–YAMLé…ç½®
                if 'docker-compose' in file_path:
                    dependencies.append('docker_orchestration')
                
            elif 'Dockerfile' in file_path:
                dependencies.append('docker_build')
        
        except (OSError, UnicodeDecodeError):
            pass
        
        return dependencies
    
    def create_cleanup_plan(self) -> Dict:
        """åˆ›å»ºæ¸…ç†è®¡åˆ’"""
        logger.info("åˆ›å»ºæ¸…ç†è®¡åˆ’...")
        
        plan = {
            'to_delete': [],
            'to_keep': [],
            'warnings': []
        }
        
        # 1. æŸ¥æ‰¾é‡å¤æ–‡ä»¶
        duplicates = self.find_duplicate_files()
        for content_hash, paths in duplicates:
            if len(paths) > 1:
                # ä¿ç•™ç¬¬ä¸€ä¸ªï¼Œåˆ é™¤å…¶ä»–
                plan['to_keep'].append(paths[0])
                for duplicate in paths[1:]:
                    plan['to_delete'].append({
                        'path': duplicate,
                        'reason': f'Duplicate of {paths[0]}'
                    })
        
        # 2. è¯†åˆ«å†—ä½™çš„Dockeræ–‡ä»¶
        redundant_docker = self.identify_redundant_docker_files()
        for docker_file in redundant_docker:
            plan['to_delete'].append({
                'path': docker_file,
                'reason': 'Redundant Docker configuration'
            })
        
        # 3. è¯†åˆ«å†—ä½™çš„æ–‡æ¡£
        redundant_docs = self.identify_redundant_docs()
        for doc_file in redundant_docs:
            plan['to_delete'].append({
                'path': doc_file,
                'reason': 'Redundant documentation'
            })
        
        # 4. è¯†åˆ«å†—ä½™çš„é…ç½®
        redundant_configs = self.identify_redundant_configs()
        for config_file in redundant_configs:
            plan['to_delete'].append({
                'path': config_file,
                'reason': 'Redundant configuration'
            })
        
        # 5. æ£€æŸ¥ç©ºæ–‡ä»¶
        analysis = self.cleanup_report.get('analysis', {})
        for empty_file in analysis.get('empty_files', []):
            if empty_file not in self.core_files and empty_file not in self.production_files:
                plan['to_delete'].append({
                    'path': empty_file,
                    'reason': 'Empty file'
                })
        
        # 6. å®‰å…¨æ£€æŸ¥ - ç¡®ä¿æ ¸å¿ƒæ–‡ä»¶ä¸è¢«åˆ é™¤
        files_to_delete = [item['path'] for item in plan['to_delete']]
        for core_file in self.core_files:
            if core_file in files_to_delete:
                plan['warnings'].append(f'Core file marked for deletion: {core_file}')
                # ä»åˆ é™¤åˆ—è¡¨ä¸­ç§»é™¤
                plan['to_delete'] = [item for item in plan['to_delete'] if item['path'] != core_file]
        
        for prod_file in self.production_files:
            if prod_file in files_to_delete:
                plan['warnings'].append(f'Production file marked for deletion: {prod_file}')
                # ä»åˆ é™¤åˆ—è¡¨ä¸­ç§»é™¤
                plan['to_delete'] = [item for item in plan['to_delete'] if item['path'] != prod_file]
        
        return plan
    
    def execute_cleanup(self, plan: Dict, dry_run: bool = True) -> bool:
        """æ‰§è¡Œæ¸…ç†è®¡åˆ’"""
        logger.info(f"æ‰§è¡Œæ¸…ç†è®¡åˆ’ (dry_run={dry_run})...")
        
        if dry_run:
            logger.info("è¿™æ˜¯ä¸€æ¬¡è¯•è¿è¡Œï¼Œä¸ä¼šå®é™…åˆ é™¤æ–‡ä»¶")
        
        success_count = 0
        error_count = 0
        
        for item in plan['to_delete']:
            file_path = self.project_root / item['path']
            
            try:
                if file_path.exists():
                    if not dry_run:
                        if file_path.is_file():
                            file_path.unlink()
                        elif file_path.is_dir():
                            shutil.rmtree(file_path)
                    
                    logger.info(f"{'[DRY RUN] ' if dry_run else ''}åˆ é™¤: {item['path']} - {item['reason']}")
                    self.cleanup_report['deleted_files'].append(item)
                    success_count += 1
                else:
                    logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨: {item['path']}")
            
            except Exception as e:
                logger.error(f"åˆ é™¤æ–‡ä»¶å¤±è´¥ {item['path']}: {e}")
                self.cleanup_report['errors'].append(f"Failed to delete {item['path']}: {e}")
                error_count += 1
        
        logger.info(f"æ¸…ç†å®Œæˆ: æˆåŠŸ {success_count}, å¤±è´¥ {error_count}")
        return error_count == 0
    
    def generate_report(self) -> str:
        """ç”Ÿæˆæ¸…ç†æŠ¥å‘Š"""
        report_path = self.project_root / 'cleanup_report.json'
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(self.cleanup_report, f, indent=2, ensure_ascii=False)
        
        # ç”Ÿæˆå¯è¯»çš„æŠ¥å‘Š
        readable_report = f"""
SSLè¯ä¹¦ç®¡ç†ç³»ç»Ÿ - ä»£ç æ¸…ç†æŠ¥å‘Š
=====================================

æ¸…ç†æ—¶é—´: {self.cleanup_report['timestamp']}

é¡¹ç›®åˆ†æç»“æœ:
- æ€»æ–‡ä»¶æ•°: {self.cleanup_report['analysis'].get('total_files', 0)}
- åˆ é™¤æ–‡ä»¶æ•°: {len(self.cleanup_report['deleted_files'])}
- ä¿ç•™æ–‡ä»¶æ•°: {len(self.cleanup_report['kept_files'])}

åˆ é™¤çš„æ–‡ä»¶:
"""
        
        for item in self.cleanup_report['deleted_files']:
            readable_report += f"  - {item['path']} ({item['reason']})\n"
        
        if self.cleanup_report['warnings']:
            readable_report += "\nè­¦å‘Š:\n"
            for warning in self.cleanup_report['warnings']:
                readable_report += f"  - {warning}\n"
        
        if self.cleanup_report['errors']:
            readable_report += "\né”™è¯¯:\n"
            for error in self.cleanup_report['errors']:
                readable_report += f"  - {error}\n"
        
        readable_report_path = self.project_root / 'cleanup_report.txt'
        with open(readable_report_path, 'w', encoding='utf-8') as f:
            f.write(readable_report)
        
        return str(readable_report_path)
    
    def run_cleanup(self, dry_run: bool = True) -> bool:
        """è¿è¡Œå®Œæ•´çš„æ¸…ç†æµç¨‹"""
        logger.info("å¼€å§‹ä»£ç æ¸…ç†æµç¨‹...")
        
        try:
            # 1. åˆ†æé¡¹ç›®ç»“æ„
            self.analyze_project_structure()
            
            # 2. åˆ›å»ºæ¸…ç†è®¡åˆ’
            plan = self.create_cleanup_plan()
            
            # 3. æ‰§è¡Œæ¸…ç†
            success = self.execute_cleanup(plan, dry_run)
            
            # 4. ç”ŸæˆæŠ¥å‘Š
            report_path = self.generate_report()
            
            logger.info(f"æ¸…ç†å®Œæˆï¼ŒæŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
            return success
            
        except Exception as e:
            logger.error(f"æ¸…ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            self.cleanup_report['errors'].append(str(e))
            return False


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='SSLè¯ä¹¦ç®¡ç†ç³»ç»Ÿä»£ç æ¸…ç†å·¥å…·')
    parser.add_argument('--project-root', default='.', help='é¡¹ç›®æ ¹ç›®å½•')
    parser.add_argument('--dry-run', action='store_true', help='è¯•è¿è¡Œï¼Œä¸å®é™…åˆ é™¤æ–‡ä»¶')
    parser.add_argument('--verbose', '-v', action='store_true', help='è¯¦ç»†è¾“å‡º')
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # è¿è¡Œæ¸…ç†
    cleaner = ProjectCleaner(args.project_root)
    
    if cleaner.run_cleanup(dry_run=args.dry_run):
        print("\nğŸ‰ ä»£ç æ¸…ç†æˆåŠŸå®Œæˆï¼")
        sys.exit(0)
    else:
        print("\nğŸ’¥ ä»£ç æ¸…ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼")
        sys.exit(1)


if __name__ == '__main__':
    main()
